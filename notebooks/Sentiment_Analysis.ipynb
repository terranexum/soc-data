{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment classification with Sentiment140 dataset\n\n1. [x] Parse data and access dataset\n2. [x] Inspect dataset structure\n3. [x] Ensure sentiment target feature encoded as binary\n4. [x] Preprocess tweets\n5. [x] Split dataset into training, validation and test sets\n6. [x] Define 3 classification models \n7. [x] Use 10-fold cross-validation to define the optimal model\n8. [x] Train best model on full training and validation sets\n7. [x] Measure final model performance on test set\n8. [x] References\n","metadata":{}},{"cell_type":"code","source":"import csv\nimport os\nimport numpy as np\nimport string\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport pickle\nimport random\nimport dask.dataframe as dd\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom zipfile import ZipFile\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nimport multiprocessing\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold, train_test_split\nnltk.download(\"stopwords\") ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:28:49.200757Z","iopub.execute_input":"2022-10-29T13:28:49.201452Z","iopub.status.idle":"2022-10-29T13:28:49.211478Z","shell.execute_reply.started":"2022-10-29T13:28:49.201415Z","shell.execute_reply":"2022-10-29T13:28:49.210527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Max cpu detected: {}'.format(multiprocessing.cpu_count()))\n#npartitions = multiprocessing.cpu_count()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:28:51.428524Z","iopub.execute_input":"2022-10-29T13:28:51.429212Z","iopub.status.idle":"2022-10-29T13:28:51.434557Z","shell.execute_reply.started":"2022-10-29T13:28:51.429176Z","shell.execute_reply":"2022-10-29T13:28:51.433638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if tf.config.list_physical_devices('GPU'):\n    print('GPU available ;)')\nelse:\n    print('No GPU detected...when it comes to training please set the accelerator to use a GPU')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:28:52.932579Z","iopub.execute_input":"2022-10-29T13:28:52.933703Z","iopub.status.idle":"2022-10-29T13:28:52.939888Z","shell.execute_reply.started":"2022-10-29T13:28:52.933656Z","shell.execute_reply":"2022-10-29T13:28:52.938818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/sentiment140/training.1600000.processed.noemoticon.csv'\n\n@dataclass\nclass CONFIG():\n  \"\"\"\n  \"\"\"\n  col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n  embedding_dim = 300\n  maxlen = 50\n  vocab_size = 200000\n  truncating = 'post'\n  padding = 'post'\n  oov_token = '<OOV>'\n  max_examples = 160000\n  training_split = .9\n\nConfig = CONFIG()\n\ndata = pd.read_csv(data_path,\n                   names = CONFIG.col_names,\n                   encoding = \"ISO-8859-1\")\n\nprint('Dataset size {}'.format(len(data)))\nprint('Dataset first five rows:\\n{}'.format(data.head()))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:28:54.763536Z","iopub.execute_input":"2022-10-29T13:28:54.763905Z","iopub.status.idle":"2022-10-29T13:29:01.349164Z","shell.execute_reply.started":"2022-10-29T13:28:54.763873Z","shell.execute_reply":"2022-10-29T13:29:01.347755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols=2,\n                               figsize = (16, 8))\n\ndata.target.value_counts().plot(kind = 'bar', ax = ax1)\nax1.set_title('Before remapping')\ndata.target = data.target.map({0 : 0,\n                               4 : 1})\ndata.target.value_counts().plot(kind = 'bar', ax = ax2)\nax2.set_title('After remapping')\nplt.tight_layout()     ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:04.151165Z","iopub.execute_input":"2022-10-29T13:29:04.151525Z","iopub.status.idle":"2022-10-29T13:29:04.6763Z","shell.execute_reply.started":"2022-10-29T13:29:04.151493Z","shell.execute_reply":"2022-10-29T13:29:04.675257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample text to check appearance of special characters\nstop_words = stopwords.words('english')\nporter = nltk.stem.PorterStemmer()\nremove_links = \"https?:\\S+|http?:\\S|[0-9]+\"\nrep_elipse = \"\\.{2,}\"\nstop_list = stop_words + list(string.punctuation)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:12.476649Z","iopub.execute_input":"2022-10-29T13:29:12.477237Z","iopub.status.idle":"2022-10-29T13:29:12.48483Z","shell.execute_reply.started":"2022-10-29T13:29:12.477194Z","shell.execute_reply":"2022-10-29T13:29:12.483828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets remove stop words first and lowercase all tokens. We will also reduce elipses to two dots, people love elipses and all their meaning!\n\nI will also utilise stemming to reduce the size of the vocabulary. Stemming truncates words to their roots, which unlike lemmatisation do not have to form valid words in the vocabulary. This will also help reduce model complexity and the size of the final model since the embedding layer will have reduced number of words to represent! Moreover, it some tweets are not english.","metadata":{}},{"cell_type":"code","source":"def preprocess_tweet(tweet: str, \n                     stemmer: object,\n                     remove_links_regex,\n                     reduce_elipses_regex):\n  \"\"\"\n  \"\"\"\n  tweet_tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True,\n                                                 reduce_len=True)\n  # remove links and numbers\n  \n  tweet = re.sub(remove_links_regex, '', str(tweet))\n  # Tokenize \n  tweet = tweet_tokenizer.tokenize(tweet)\n  # Remove stop words\n  tweet = [stemmer.stem(token.lower().strip()) for token in tweet if token not in stop_list]\n  tweet =  ' '.join(tweet)\n\n  # Replace elipse (two or more .. with ..)\n  tweet = re.sub(reduce_elipses_regex, '..', str(tweet))\n  \n  return tweet","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:16.420431Z","iopub.execute_input":"2022-10-29T13:29:16.42078Z","iopub.status.idle":"2022-10-29T13:29:16.427716Z","shell.execute_reply.started":"2022-10-29T13:29:16.42075Z","shell.execute_reply":"2022-10-29T13:29:16.426553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = data.head()\ntest = test['text'].apply(lambda x: preprocess_tweet(x, porter, remove_links, rep_elipse))\nprint('Before: {}'. format(list(data['text'][:5])))\nprint('After: {}'. format(list(test[:5])))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:18.501901Z","iopub.execute_input":"2022-10-29T13:29:18.502373Z","iopub.status.idle":"2022-10-29T13:29:18.526837Z","shell.execute_reply.started":"2022-10-29T13:29:18.502332Z","shell.execute_reply":"2022-10-29T13:29:18.52579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_tweet_df(dd:object):\n  \"\"\"\n  \"\"\"\n  dd['text'] = data['text'].map(lambda tweet: preprocess_tweet(tweet, \n                                                               porter,\n                                                               remove_links,\n                                                               rep_elipse))\n\n  return dd","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:20.238408Z","iopub.execute_input":"2022-10-29T13:29:20.238765Z","iopub.status.idle":"2022-10-29T13:29:20.243887Z","shell.execute_reply.started":"2022-10-29T13:29:20.238733Z","shell.execute_reply":"2022-10-29T13:29:20.242956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets remove user, links and punctuation.","metadata":{}},{"cell_type":"code","source":"print('Some stop word examples ... {}'.format(list(stop_words[:10])))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:22.357833Z","iopub.execute_input":"2022-10-29T13:29:22.35908Z","iopub.status.idle":"2022-10-29T13:29:22.364884Z","shell.execute_reply.started":"2022-10-29T13:29:22.359034Z","shell.execute_reply":"2022-10-29T13:29:22.363735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop unnecessary columns","metadata":{}},{"cell_type":"code","source":"data = data.drop(['ids', 'date', 'flag','user'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:24.052239Z","iopub.execute_input":"2022-10-29T13:29:24.052706Z","iopub.status.idle":"2022-10-29T13:29:24.204109Z","shell.execute_reply.started":"2022-10-29T13:29:24.052654Z","shell.execute_reply":"2022-10-29T13:29:24.203052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y  = data['text'].tolist(), data.pop('target').to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:25.523235Z","iopub.execute_input":"2022-10-29T13:29:25.523591Z","iopub.status.idle":"2022-10-29T13:29:25.564814Z","shell.execute_reply.started":"2022-10-29T13:29:25.523561Z","shell.execute_reply":"2022-10-29T13:29:25.563783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = [preprocess_tweet(tweet,porter,remove_links,rep_elipse) for tweet in X]","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:29:27.227561Z","iopub.execute_input":"2022-10-29T13:29:27.22793Z","iopub.status.idle":"2022-10-29T13:38:17.305905Z","shell.execute_reply.started":"2022-10-29T13:29:27.227896Z","shell.execute_reply":"2022-10-29T13:38:17.304895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare list of X, y tuples, which I will save to disk to save time later on.","metadata":{}},{"cell_type":"code","source":"processed = [(X[i], y[i]) for i, tweet in enumerate(X)]","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:44:06.73444Z","iopub.execute_input":"2022-10-29T13:44:06.734815Z","iopub.status.idle":"2022-10-29T13:44:07.274607Z","shell.execute_reply.started":"2022-10-29T13:44:06.734784Z","shell.execute_reply":"2022-10-29T13:44:07.27366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Post preprocessing check: {}'.format(list(X[:5])))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:44:08.759937Z","iopub.execute_input":"2022-10-29T13:44:08.761002Z","iopub.status.idle":"2022-10-29T13:44:08.767082Z","shell.execute_reply.started":"2022-10-29T13:44:08.760966Z","shell.execute_reply":"2022-10-29T13:44:08.765627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/sentiment140_processed.pkl', 'wb') as f:\n  pickle.dump(processed, f)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:44:21.178058Z","iopub.execute_input":"2022-10-29T13:44:21.178515Z","iopub.status.idle":"2022-10-29T13:44:26.638918Z","shell.execute_reply.started":"2022-10-29T13:44:21.178475Z","shell.execute_reply":"2022-10-29T13:44:26.637827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load preprocess tweets (easier to start from here if you already save your preprocessed data !) ","metadata":{}},{"cell_type":"code","source":"# Load processed tweets with target data\nwith open('/kaggle/working/sentiment140_processed.pkl', 'rb') as f:\n  processed = pickle.load(f)\n\nprint('Processed data: {}'.format(processed[0]))\n\nX, y = zip(*processed)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:45:46.358701Z","iopub.execute_input":"2022-10-29T13:45:46.359462Z","iopub.status.idle":"2022-10-29T13:45:50.307952Z","shell.execute_reply.started":"2022-10-29T13:45:46.359424Z","shell.execute_reply":"2022-10-29T13:45:50.306852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X))\nprint(len(y))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:45:51.874348Z","iopub.execute_input":"2022-10-29T13:45:51.875089Z","iopub.status.idle":"2022-10-29T13:45:51.880831Z","shell.execute_reply.started":"2022-10-29T13:45:51.875051Z","shell.execute_reply":"2022-10-29T13:45:51.87966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split into train/val and test sets.","metadata":{}},{"cell_type":"code","source":"# Split in training validation and test sets\nX_train, X_test, y_train, y_test = test = train_test_split(X, y,\n                                                           shuffle = True, \n                                                           random_state=1, \n                                                           test_size = 10000,\n                                                           stratify = y)\n\nprint('Train/val size is {}'.format(len(X_train))) \nprint('Test size is {}'.format(len(X_test))) \nprint('Example train/val tweet: {}'.format(X_train[:1]))\nprint('Example test tweet: {}'.format(X_test[:1]))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:45:54.702525Z","iopub.execute_input":"2022-10-29T13:45:54.702893Z","iopub.status.idle":"2022-10-29T13:45:56.209922Z","shell.execute_reply.started":"2022-10-29T13:45:54.702856Z","shell.execute_reply":"2022-10-29T13:45:56.208912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenise the tweets!","metadata":{}},{"cell_type":"code","source":"def fit_tokenizer(train_sentences, oov_token, vocab_size):\n  \"\"\"\n  \"\"\"\n\n  tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\n\n  tokenizer.fit_on_texts(train_sentences)\n\n  return tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:46:02.053856Z","iopub.execute_input":"2022-10-29T13:46:02.054215Z","iopub.status.idle":"2022-10-29T13:46:02.059773Z","shell.execute_reply.started":"2022-10-29T13:46:02.054178Z","shell.execute_reply":"2022-10-29T13:46:02.058832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instantiate tokenizer!","metadata":{}},{"cell_type":"code","source":"tokenizer = fit_tokenizer(X, oov_token = Config.oov_token, vocab_size = Config.vocab_size)\n\nword_index = tokenizer.word_index\n\nprint('Vocab contains {} words'.format(Config.vocab_size))\nprint('<OOV> token successfully placed in vocabulary!' if '<OOV>' in word_index else 'No <OOV> in vocabulary! something went wrong :(')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:46:03.735659Z","iopub.execute_input":"2022-10-29T13:46:03.736019Z","iopub.status.idle":"2022-10-29T13:46:25.806575Z","shell.execute_reply.started":"2022-10-29T13:46:03.735988Z","shell.execute_reply":"2022-10-29T13:46:25.805535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check tweet length and use this data to inform our choice on tweet length.","metadata":{}},{"cell_type":"code","source":"tweet_lengths = [len(tweet) for tweet in X]\nplt.figure(figsize=(16,8))\nplt.hist(tweet_lengths, \n         bins = 100)\nplt.axvline( Config.maxlen, \n            ls = '--',\n            c = 'red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:46:27.921853Z","iopub.execute_input":"2022-10-29T13:46:27.922496Z","iopub.status.idle":"2022-10-29T13:46:33.06676Z","shell.execute_reply.started":"2022-10-29T13:46:27.922461Z","shell.execute_reply":"2022-10-29T13:46:33.065559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenise_sentences(tweets: list,\n                       tokenizer: object,\n                       padding: str,\n                       truncating: str,\n                       maxlen: int):\n  \"\"\"\n  \"\"\"\n\n  tweets = tokenizer.texts_to_sequences(tweets)\n\n  padded_and_trunc_tweets = pad_sequences(sequences = tweets,\n                                          maxlen = maxlen,\n                                          truncating = truncating,\n                                          padding = padding)\n  \n  return padded_and_trunc_tweets\n\n# Try increasing the maxlen ;)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:47:11.65631Z","iopub.execute_input":"2022-10-29T13:47:11.656659Z","iopub.status.idle":"2022-10-29T13:47:11.662451Z","shell.execute_reply.started":"2022-10-29T13:47:11.656628Z","shell.execute_reply":"2022-10-29T13:47:11.661352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = tokenise_sentences(X_train, tokenizer, Config.padding, Config.truncating, Config.maxlen) \nX_test = tokenise_sentences(X_test, tokenizer, Config.padding, Config.truncating, Config.maxlen) ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:47:13.30069Z","iopub.execute_input":"2022-10-29T13:47:13.3014Z","iopub.status.idle":"2022-10-29T13:47:39.872983Z","shell.execute_reply.started":"2022-10-29T13:47:13.301362Z","shell.execute_reply":"2022-10-29T13:47:39.871769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check dimensions of padded tokenised tweet sequences","metadata":{}},{"cell_type":"code","source":"print('Tokenised tweets have shape {}'.format(X_train.shape))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:47:42.884039Z","iopub.execute_input":"2022-10-29T13:47:42.884991Z","iopub.status.idle":"2022-10-29T13:47:42.890784Z","shell.execute_reply.started":"2022-10-29T13:47:42.884943Z","shell.execute_reply":"2022-10-29T13:47:42.889814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets define a model that uses leverages an embedding layer. I'll try 3 architectures, some of which are included for explanatory purposes.","metadata":{}},{"cell_type":"code","source":"def sentiment_classifier_embedding_max(vocab_size, embedding_dim, maxlen):\n  \"\"\"\n  \"\"\"\n  model = tf.keras.Sequential([\n                               tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n                               tf.keras.layers.GlobalMaxPooling1D(),\n                               tf.keras.layers.Dropout(.3),\n                               tf.keras.layers.Dense(32, activation = tf.nn.relu),\n                               tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n  ])\n\n  model.compile(loss = 'binary_crossentropy',\n                optimizer = tf.keras.optimizers.Adam(0.001),\n                metrics = ['acc'])\n  \n  return model","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:47:49.517065Z","iopub.execute_input":"2022-10-29T13:47:49.518122Z","iopub.status.idle":"2022-10-29T13:47:49.525245Z","shell.execute_reply.started":"2022-10-29T13:47:49.518074Z","shell.execute_reply":"2022-10-29T13:47:49.523977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_classifier_lstm(vocab_size, embedding_dim, maxlen):\n  \"\"\"\n  \"\"\"\n  model = tf.keras.Sequential([\n                               tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n                               tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n                               tf.keras.layers.Dropout(.4),\n                               tf.keras.layers.Dense(32, activation = tf.nn.relu),\n                               tf.keras.layers.Dropout(.4),\n                               tf.keras.layers.Dense(16, activation = tf.nn.relu),\n                               tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n  ])\n\n  model.compile(loss = 'binary_crossentropy',\n                optimizer = tf.keras.optimizers.Adam(0.001),\n                metrics = ['acc'])\n  \n  return model","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:47:51.479654Z","iopub.execute_input":"2022-10-29T13:47:51.480032Z","iopub.status.idle":"2022-10-29T13:47:51.487867Z","shell.execute_reply.started":"2022-10-29T13:47:51.48Z","shell.execute_reply":"2022-10-29T13:47:51.486821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_classifier_lstm(Config.vocab_size, Config.embedding_dim, Config.maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:47:53.023808Z","iopub.execute_input":"2022-10-29T13:47:53.024822Z","iopub.status.idle":"2022-10-29T13:47:56.445597Z","shell.execute_reply.started":"2022-10-29T13:47:53.024783Z","shell.execute_reply":"2022-10-29T13:47:56.444466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model checkpoint callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=1,verbose=0,mode='auto')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:51:40.864975Z","iopub.execute_input":"2022-10-29T13:51:40.86534Z","iopub.status.idle":"2022-10-29T13:51:40.872553Z","shell.execute_reply.started":"2022-10-29T13:51:40.865308Z","shell.execute_reply":"2022-10-29T13:51:40.870812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to evaluate each architecture i'll need to train for a reasonable number of epochs and then recover the model with the best validation loss observed, it would not be fair to compare models at the end of all the epochs are some overfit and others might not. The accuracy will then be averaged over each validation fold and we will then be able to compare model performance. The early stopping will ensure models do not overfit prior to comparison, whilst the 'Reduce LR on plateau' will give the models a chance to optimise their parameters further.\n\nGet train test split\n\n---","metadata":{}},{"cell_type":"code","source":"y_train = np.array(y_train)\ny_test = np.array(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:51:44.036577Z","iopub.execute_input":"2022-10-29T13:51:44.036972Z","iopub.status.idle":"2022-10-29T13:51:44.044044Z","shell.execute_reply.started":"2022-10-29T13:51:44.03694Z","shell.execute_reply":"2022-10-29T13:51:44.042973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below cross validation will take a while if you wanting to compare architectures through a number of epochs","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=3, \n           shuffle=True, \n           random_state=1)\n\nfold = 0\n\n# takes a while to train ...\nepochs = 3\n\nmodel_eval = defaultdict(list)\n# 10 fold cross validation strategy\nfor train_index, test_index in kf.split(X_train):\n    cv_X_train, cv_X_val= X_train[train_index], X_train[test_index]\n    cv_y_train, cv_y_val = y_train[train_index], y_train[test_index]\n    print('Fold: {}'.format(fold))\n    print('# train: {}\\n# val: {}'.format(len(train_index), len(test_index)))\n\n    # reinitialise classifers to reset weights\n    sentiment_classifier_max_model = sentiment_classifier_embedding_max(Config.vocab_size, Config.embedding_dim, Config.maxlen)\n    sentiment_classifier_lstm_model = sentiment_classifier_lstm(Config.vocab_size, Config.embedding_dim, Config.maxlen)\n\n    # train model 1\n    history_max = sentiment_classifier_max_model.fit(cv_X_train,\n                                             cv_y_train,\n                                             validation_data = (cv_X_val, cv_y_val),\n                                             callbacks = [early_stopping, reduce_lr],\n                                             epochs = epochs,\n                                             batch_size = 128)\n    # train model 2\n    history_lstm = sentiment_classifier_lstm_model.fit(cv_X_train,\n                                               cv_y_train,\n                                               validation_data = (cv_X_val, cv_y_val),\n                                               callbacks = [early_stopping, reduce_lr],\n                                               epochs = epochs,\n                                               batch_size = 128)\n\n\n    # make predictions on hold out validation set for each model\n    _ , acc = sentiment_classifier_max_model.evaluate(cv_X_val, cv_y_val)\n    _ , acc2 = sentiment_classifier_lstm_model.evaluate(cv_X_val, cv_y_val)\n\n    # calculate performance metric\n    model_eval['sentiment_classifier_embedding_max'].append(acc)\n    model_eval['sentiment_classifier_embedding_lstm'].append(acc2)\n\n    fold += 1\n\n\nperformance1 = np.mean(model_eval['sentiment_classifier_embedding_max']) - np.std(model_eval['sentiment_classifier_embedding_max'])\nperformance2 = np.mean(model_eval['sentiment_classifier_embedding_lstm']) - np.std(model_eval['sentiment_classifier_embedding_lstm'])","metadata":{"execution":{"iopub.status.busy":"2022-10-29T13:51:46.805362Z","iopub.execute_input":"2022-10-29T13:51:46.805725Z","iopub.status.idle":"2022-10-29T14:54:55.362803Z","shell.execute_reply.started":"2022-10-29T13:51:46.805694Z","shell.execute_reply":"2022-10-29T14:54:55.361895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Performance of Embedding max model: {}'.format(performance1))\nprint('Performance of LSTM model: {}'.format(performance2))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T14:57:21.675405Z","iopub.execute_input":"2022-10-29T14:57:21.675763Z","iopub.status.idle":"2022-10-29T14:57:21.681929Z","shell.execute_reply.started":"2022-10-29T14:57:21.675732Z","shell.execute_reply":"2022-10-29T14:57:21.680676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like LSTM model just won out by a fraction.\n\nIt would be worth doing error analysis now and figuring out why the model is failing at the remaining ~20%. It could be a language issue for example, or perhaps for a fraction of the tweets it is hard to discern whether they are positive or negative because they are neutral.\n\nNow that we have used 3 fold cross validation to select the better architecture, lets go ahead and tweak the initial learning rate to push performance a little. Please do not try and run the above cell. I just wanted to relate how to integrate cross validation with tensorflow workflows as I find it really helpful. None of the deeplearning courses I have attended mention how to do this. To extend the above code, I would create a function to loop through x models and collect the performance metrics. \n\nThe reason I take away the std of the accuracy is so models are penalised by the variability of their accuracy  in each validation set. This helps choose a robust model.\n\nIts worth noting that keras has a [tuner module](https://keras.io/keras_tuner/) that could help you check how the size of the layers affects performance and to optimise these hyperparameters.\n\n---\n\n## Tuning the initial learning rate\n\n**Keras callbacks enable us to automate the selection of the starting learning rate** since we can modify it at the end of a batch and monitor how learning rates impact the cost. Here, after every epoch we tweak the learning rate to see which performs well for the task.\n\nThe formula specified below will increase the learning rate gradually so that the learning rate will ✖10 every 5 epochs.","metadata":{}},{"cell_type":"code","source":"def sentiment_classifier_lstm(vocab_size, embedding_dim, maxlen):\n  \"\"\"\n  \"\"\"\n  model = tf.keras.Sequential([\n                               tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n                               tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n                               tf.keras.layers.Dropout(.4),\n                               tf.keras.layers.Dense(32, activation = tf.nn.relu),\n                               tf.keras.layers.Dropout(.4),\n                               tf.keras.layers.Dense(16, activation = tf.nn.relu),\n                               tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n  ])\n\n  model.compile(loss = 'binary_crossentropy',\n                optimizer = tf.keras.optimizers.Adam(),\n                metrics = ['acc'])\n  \n  return model","metadata":{"execution":{"iopub.status.busy":"2022-10-29T15:00:06.526015Z","iopub.execute_input":"2022-10-29T15:00:06.526598Z","iopub.status.idle":"2022-10-29T15:00:06.533986Z","shell.execute_reply.started":"2022-10-29T15:00:06.526563Z","shell.execute_reply":"2022-10-29T15:00:06.532977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 5))\n\ntune_model = sentiment_classifier_lstm(Config.vocab_size, Config.embedding_dim, Config.maxlen)\n# Since the dataset is large ... lets reduce the dataset size\nhistory = tune_model.fit(cv_X_train[:100000],\n                         cv_y_train[:100000],\n                         validation_data = (cv_X_val[:100000], cv_y_val[:100000]),\n                         callbacks = [lr_schedule],\n                         epochs = 30, \n                         batch_size = 128)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T15:00:10.736093Z","iopub.execute_input":"2022-10-29T15:00:10.736443Z","iopub.status.idle":"2022-10-29T15:11:35.435784Z","shell.execute_reply.started":"2022-10-29T15:00:10.736412Z","shell.execute_reply":"2022-10-29T15:11:35.434746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets inspect how the loss varied with various learning rates and use the plot to 'eye-ball' an acceptable initial learning rate ⚡","metadata":{}},{"cell_type":"code","source":"learning_rates = 1e-8 * (10 ** ((np.arange(30))/5))\nbest_lr = learning_rates[np.argmin(history.history['loss'])]\nplt.figure(figsize = (10,8))\nplt.grid(True)\nplt.semilogx(learning_rates, history.history[\"loss\"])\nplt.tick_params('both', length=8, width=1, which = 'both')\nplt.axis([1e-8,.007,0,.79])\n\nplt.axvline(learning_rates[np.argmin(history.history['loss'])],\n            ls = '--',\n            c = 'red')\nplt.title('Optimising learning rate selection')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T15:13:06.267322Z","iopub.execute_input":"2022-10-29T15:13:06.267671Z","iopub.status.idle":"2022-10-29T15:13:07.154892Z","shell.execute_reply.started":"2022-10-29T15:13:06.267641Z","shell.execute_reply":"2022-10-29T15:13:07.153765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Looks like the stable sweet spot is between 10^-5 and 10^-4 . The loss starts to increase dramatically when it is above 10^-4.\n\n## Final model architecture and training\n\nAfter cross-validation, its important to re-train using both the training and validation data. **Remember to only do this once you have picked your final model based on the validation metrics**","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_classifier_lstm(vocab_size, embedding_dim, maxlen, best_lr):\n  \"\"\"\n  \"\"\"\n  model = tf.keras.Sequential([\n                               tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n                               tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, name = 'lstm_layer')),\n                               tf.keras.layers.Dropout(.4),\n                               tf.keras.layers.Dense(32, activation = tf.nn.relu, name = 'dense_layer_1'),\n                               tf.keras.layers.Dropout(.4),\n                               tf.keras.layers.Dense(16, activation = tf.nn.relu,name = 'dense_layer_2'),\n                               tf.keras.layers.Dense(1, activation = tf.nn.sigmoid,name = 'output_layer')\n  ])\n\n  model.compile(loss = 'binary_crossentropy',\n                optimizer = tf.keras.optimizers.Adam(best_lr),\n                metrics = ['acc'])\n  \n  return model","metadata":{"execution":{"iopub.status.busy":"2022-10-29T15:13:22.940412Z","iopub.execute_input":"2022-10-29T15:13:22.940767Z","iopub.status.idle":"2022-10-29T15:13:22.94864Z","shell.execute_reply.started":"2022-10-29T15:13:22.940736Z","shell.execute_reply":"2022-10-29T15:13:22.947711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have our final model architecture and one hyperparameter chosen! Lets train on the both the train and validation before testing its final performance on the test set. Once val loss starts to plateau I will reduce the learning rate if it stagnates over 3 epochs. When you want to save models, the modelcheckpoint callback is superb, and you can set it to only save to best model! ♥","metadata":{}},{"cell_type":"code","source":"final_model = sentiment_classifier_lstm(Config.vocab_size, Config.embedding_dim, Config.maxlen, best_lr)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=0,mode='auto')\n\n# The below will only save if the val accuracy improves. This means we can later load the best model without worrying about overtraining.\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/kaggle/working/sentiment140-lstm-model_{epoch:02d}_{acc:.2f}',\n                                                               save_weights_only=False,\n                                                               monitor='val_acc',\n                                                               mode='max',\n                                                               save_best_only=True)\nhistory = final_model.fit(X_train[:-5000],\n                         y_train[:-5000],\n                         validation_data = (X_train[-5000:], y_train[-5000:]),\n                         callbacks = [reduce_lr,model_checkpoint_callback],\n                         epochs = 10, \n                         batch_size = 128)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T15:30:24.614142Z","iopub.execute_input":"2022-10-29T15:30:24.614509Z","iopub.status.idle":"2022-10-29T16:20:14.107121Z","shell.execute_reply.started":"2022-10-29T15:30:24.614476Z","shell.execute_reply":"2022-10-29T16:20:14.106203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load final model and check final performance ⭐","metadata":{}},{"cell_type":"code","source":"final_model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T16:53:42.264814Z","iopub.execute_input":"2022-10-29T16:53:42.265284Z","iopub.status.idle":"2022-10-29T16:53:43.859444Z","shell.execute_reply.started":"2022-10-29T16:53:42.265247Z","shell.execute_reply":"2022-10-29T16:53:43.858558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ~ 80 % great!\n\nIn a few months I hope to do error analysis and understand the failings of this model. Obviously this should be done before the test set is used to evaluate the model, but this is kaggle and I am only doing it for curiousity!","metadata":{}},{"cell_type":"markdown","source":"# Closing remarks\n\nI hope you found this notebook useful and maybe learnt a neat trick or two! Please leave any constructive feedback on avenues I could explore to easily improve this notebook.\n\nHappy Kaggling ✌","metadata":{}}]}